{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# GILL Inference Examples\n",
    "\n",
    "This is a notebook showcasing how to run GILL for image generation, image retrieval, and text generation, some of the tasks that GILL is capable of. It reproduces several examples in our paper, [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216).\n",
    "\n",
    "For reproducibility, all examples in this notebook use greedy (deterministic) decoding. However, it is possible to change to nucleus sampling for more diverse and higher quality outputs (used for some of the figures in the paper) by changing the `temperature` and `top_p` parameters in the `generate()` function.\n",
    "\n",
    "At least 22GB of GPU memory is required to run this model, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import notebook\n",
    "\n",
    "from gill import models\n",
    "from gill import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f36e5f-7027-4e60-95d5-05aa43ee34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1463559",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Note that you will need to download the [CC3M image embeddings](https://drive.google.com/file/d/1e9Cimh2dpWN8Cbgx_mSR-954Dr-DS-ZO/view) and place them in the `checkpoints/gill_opt/` folder, in order to use GILL's image retrieval capabilities. If this embedding file does not exist, the model will still run, but it will exclusively generate images (as opposed to deciding when to retrieve or generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc3m.npy files do not exist in checkpoints/gill_opt/.\n",
      "Running the model without retrieval.\n",
      "Adding [IMG0] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG0]\") = {'input_ids': [10975, 3755, 534, 288, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG0]\") = {'input_ids': [50266], 'attention_mask': [1]}\n",
      "Adding [IMG1] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG1]\") = {'input_ids': [10975, 3755, 534, 134, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG1]\") = {'input_ids': [50267], 'attention_mask': [1]}\n",
      "Adding [IMG2] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG2]\") = {'input_ids': [10975, 3755, 534, 176, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG2]\") = {'input_ids': [50268], 'attention_mask': [1]}\n",
      "Adding [IMG3] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG3]\") = {'input_ids': [10975, 3755, 534, 246, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG3]\") = {'input_ids': [50269], 'attention_mask': [1]}\n",
      "Adding [IMG4] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG4]\") = {'input_ids': [10975, 3755, 534, 306, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG4]\") = {'input_ids': [50270], 'attention_mask': [1]}\n",
      "Adding [IMG5] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG5]\") = {'input_ids': [10975, 3755, 534, 245, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG5]\") = {'input_ids': [50271], 'attention_mask': [1]}\n",
      "Adding [IMG6] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG6]\") = {'input_ids': [10975, 3755, 534, 401, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG6]\") = {'input_ids': [50272], 'attention_mask': [1]}\n",
      "Adding [IMG7] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG7]\") = {'input_ids': [10975, 3755, 534, 406, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG7]\") = {'input_ids': [50273], 'attention_mask': [1]}\n",
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/media/wiseyak/reasoning/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for laion/clap-htsat-fused.\n",
      "Using facebook/opt-125m for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 4 visual tokens.\n",
      "Using laion/clap-htsat-fused as audio encoder\n",
      "Freezing the audio model\n",
      "Freezing the LM.\n",
      "Restoring pretrained weights for the visual model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_projection.weight', 'text_model.encoder.layers.0.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/media/wiseyak/reasoning/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the VM.\n",
      "---------- Audio embedding ----------\n",
      "LM input embedding dimension :  768\n",
      "number of visual tokens :  4\n",
      "Embedding dim:  3072\n",
      "Retrieval embedding dim:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decision model...\n"
     ]
    }
   ],
   "source": [
    "# Download the model checkpoint and embeddings to checkpoints/gill_opt/\n",
    "model_dir = 'checkpoints/gill_opt/'\n",
    "model = models.load_gill(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040de82-8305-4587-a340-4ae95401c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caption\n",
    "encoder_outputs = torch.rand((1,1024))\n",
    "hidden_size = 1024\n",
    "\n",
    "visual_embeddings = nn.Linear(hidden_size, 16384)\n",
    "visual_embs = visual_embeddings(encoder_outputs)\n",
    "visual_embs = torch.reshape(visual_embs, (visual_embs.shape[0], 4, -1))\n",
    "visual_embs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e575ef-11aa-4b4a-a617-aaaf789d45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retireval\n",
    "visual_fc = nn.Linear(hidden_size, 256)\n",
    "visual_embs = visual_fc(encoder_outputs)\n",
    "print(visual_embs.size())\n",
    "visual_embs = torch.reshape(visual_embs, (visual_embs.shape[0], 1, -1))\n",
    "print(visual_embs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66584911",
   "metadata": {},
   "source": [
    "# Image Generation\n",
    "\n",
    "GILL can generate images conditioned on image and text inputs. Shown are several examples for various text prompts and image + text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c9fa0-e417-447f-bfbe-d783d39a2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "red = utils.get_image_from_url('https://unsplash.com/photos/3TuIIkWlpvA/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8cmVkJTIwY29sb3J8ZW58MHx8fHwxNjkxOTk1NzM0fDA&force=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5eb4-10c0-4d10-96cc-2af5ff64909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# response = requests.get('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "# ['an astronaut riding a horse on mars'],\n",
    "    # [sofa_img, 'a picture of this but in red, color'],\n",
    "\n",
    "# Generate for a few types of text prompts and image + text prompts.\n",
    "for prompt in [\n",
    "    ['a picture of a cat'],    \n",
    "]:\n",
    "    g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "    return_outputs = model.generate_for_images_and_texts(\n",
    "        prompt, num_words=2, ret_scale_factor=100.0, generator=g_cuda)\n",
    "    \n",
    "    # Show either the generated or retrieved image, depending on the decision model outputs.\n",
    "    if return_outputs[1]['decision'][0] == 'gen':\n",
    "        plt.imshow(return_outputs[1]['gen'][0][0])\n",
    "        plt.title('Generated')\n",
    "    else:\n",
    "        plt.imshow(return_outputs[1]['ret'][0][0].resize((512, 512)))\n",
    "        plt.title(f\"Retrieved\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92172951",
   "metadata": {},
   "source": [
    "# Caption an audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a386f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_data, sampling_rate = librosa.load(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c693ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "type(audio_data) in [np.ndarray,torch.Tensor,List[np.ndarray],List[torch.Tensor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fce2bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Audio embs shape (captioning) : torch.Size([1, 4, 768])\n",
      "<class 'str'>\n",
      " a man who has been in a coma for a year.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    audio_data,\n",
    "    'This is the sound of'\n",
    "]\n",
    "\n",
    "return_outputs = model.generate_for_images_and_texts(prompts, num_words=16, min_word_tokens=16)\n",
    "print(return_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee0ce",
   "metadata": {},
   "source": [
    "# Multimodal Dialogue\n",
    "\n",
    "GILL can also generate dialogue-like text. We define some helper functions for displaying outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3a339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_dialogue(prompts: list, system_message: str = None, num_words: int = 32,\n",
    "                      sf: float = 1.0, temperature: float = 0.0, top_p: float = 1.0,\n",
    "                      divider_count: int = 40):\n",
    "    g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "\n",
    "    full_outputs = []\n",
    "    if system_message:\n",
    "        print(\"Adding system message\")\n",
    "        full_inputs = [system_message]\n",
    "    else:\n",
    "        full_inputs = []\n",
    "\n",
    "    for prompt_idx, prompt in notebook.tqdm(enumerate(prompts), total=len(prompts)):\n",
    "        formatted_prompt = []\n",
    "        for p in prompt:\n",
    "            if type(p) == Image.Image:\n",
    "                full_inputs.append(p)\n",
    "                formatted_prompt.append(p)\n",
    "            elif type(p) == str:\n",
    "                full_inputs.append(f'Q: {p}\\nA:')\n",
    "                formatted_prompt.append(f'User: {p}')\n",
    "        formatted_prompt.append('=' * divider_count)  # Add divider\n",
    "\n",
    "        return_outputs = model.generate_for_images_and_texts(\n",
    "            full_inputs, num_words=num_words, ret_scale_factor=sf,\n",
    "            generator=g_cuda, temperature=temperature, top_p=top_p)\n",
    "\n",
    "        # Add outputs\n",
    "        output_text = return_outputs[0].replace('[IMG0] [IMG1] [IMG2] [IMG3] [IMG4] [IMG5] [IMG6] [IMG7]', '')\n",
    "        full_inputs.append(output_text + '\\n')\n",
    "\n",
    "        formatted_return_outputs = []\n",
    "        for p in return_outputs:\n",
    "            if type(p) == str:\n",
    "                p_formatted = p.replace('[IMG0] [IMG1] [IMG2] [IMG3] [IMG4] [IMG5] [IMG6] [IMG7]', '')\n",
    "                formatted_return_outputs.append(f'GILL: {p_formatted}')\n",
    "            else:\n",
    "                formatted_return_outputs.append(p)\n",
    "        formatted_return_outputs.append('=' * divider_count)  # Add divider\n",
    "\n",
    "        full_outputs.extend(formatted_prompt + formatted_return_outputs)\n",
    "\n",
    "    return full_outputs\n",
    "\n",
    "\n",
    "def display_conversation(full_outputs):\n",
    "    # Display conversation.\n",
    "    for p in full_outputs:\n",
    "        if type(p) == Image.Image:\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(p)\n",
    "            plt.show()\n",
    "        elif type(p) == str:\n",
    "            print(p)\n",
    "        elif type(p) == dict:\n",
    "            # Decide whether to retrieve or generate\n",
    "            decision_probs = [f'{s:.3f}' for s in p['decision'][1]]\n",
    "            if p['decision'][0] == 'gen':\n",
    "                gen_img = p['gen'][0][0].resize((512, 512))\n",
    "                # Generate\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.imshow(gen_img)\n",
    "                plt.title(f'GENERATED (p={decision_probs})')\n",
    "            else:\n",
    "                ret_img = p['ret'][0][0].resize((512, 512))\n",
    "                # Retrieve\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.imshow(ret_img)\n",
    "                plt.title(f'RETRIEVED (p={decision_probs})')\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise NotImplementedError(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73049d9b",
   "metadata": {},
   "source": [
    "The inputs to the model can be an interleaved image and text sequence. Shown is one example from our paper, with a cupcake image input and a question (note that the word \"cupcake\" is never explicitly mentioned, but GILL infers it from the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        utils.get_image_from_url('https://www.allrecipes.com/thmb/riDYvmalWk8QgJDBT_pZRkpfpR0=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/17377-chocolate-cupcakes-DDMFS-4x3-622a7a66fcd84692947794ed385dc991.jpg'),\n",
    "        'How should I publicise these at the market?'\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e5c28",
   "metadata": {},
   "source": [
    "# Image-to-Text Example\n",
    "\n",
    "GILL can also generate text conditioned on image and text inputs. This is helpful for tasks such as image captioning or VQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87889b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pancakes_img = utils.get_image_from_url('https://images.pexels.com/photos/376464/pexels-photo-376464.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "prompts = [\n",
    "    pancakes_img,\n",
    "    'A picture of'\n",
    "]\n",
    "\n",
    "plt.imshow(pancakes_img)\n",
    "plt.show()\n",
    "\n",
    "return_outputs = model.generate_for_images_and_texts(prompts, num_words=16, min_word_tokens=16)\n",
    "print(return_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8ee5e-a37a-4b77-b638-0508335e5af7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interleaving Image and Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb33973-51e8-46db-bf9d-76c85de77667",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "pancake = utils.get_image_from_url('https://images.pexels.com/photos/376464/pexels-photo-376464.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "whipped_cream = utils.get_image_from_url(\"https://media.istockphoto.com/id/510175498/photo/whipped-cream-on-white-background.jpg?s=2048x2048&w=is&k=20&c=Z_4UJPvUq2kmQXnWngY6apzKZjDgRIGfxaa82mPakcM=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7e32a-671c-45ab-80e3-c366811bcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [pancake, \"This but with grey colored syrup\"]\n",
    "# g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "# return_outputs = model.generate_for_images_and_texts(prompts, num_words=2, ret_scale_factor=100.0, generator=g_cuda)\n",
    "\n",
    "# # Show either the generated or retrieved image, depending on the decision model outputs.\n",
    "# if return_outputs[1]['decision'][0] == 'gen':\n",
    "#     plt.imshow(return_outputs[1]['gen'][0][0])\n",
    "#     plt.title('Generated')\n",
    "# else:\n",
    "#     plt.imshow(return_outputs[1]['ret'][0][0].resize((512, 512)))\n",
    "#     plt.title(f\"Retrieved\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46979494-610b-4ef7-afa9-4a215d22f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "red = utils.get_image_from_url('https://unsplash.com/photos/3TuIIkWlpvA/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8cmVkJTIwY29sb3J8ZW58MHx8fHwxNjkxOTk1NzM0fDA&force=true')\n",
    "\n",
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        sofa_img, \" but in \", blue, \" color.\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb8b85-4a79-4227-a9d9-fa07088a30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        \"How would this \", pancake, \" look with \", whipped_cream, \" on it?\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647dd5a5-6547-4825-a353-9f44bd6b2eef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VIST eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a19add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://visionandlanguage.net/VIST/json_files/story-in-sequence/SIS-with-labels.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d61d20-8e4f-457f-a877-42a5b115a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf SIS-with-labels.tar.gz -C sis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98711c54-8b0f-4b64-aabf-fe31c4688c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm SIS-with-labels.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a868cc-990d-4aaa-a8a9-711832e5893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evals/download_vist_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72dd40-df6a-449c-ab1a-29e29949f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "%%writefile output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb466c1-9e9d-4275-b297-ad670292b779",
   "metadata": {},
   "source": [
    "## Audio Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d12e3-c267-4686-ac6f-e53051fa15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from transformers import ClapModel, ClapProcessor, ClapConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3505760-8857-40e4-ac38-b929f166efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feba2bb-c458-48e5-8c3a-5f174d3ab9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration = ClapConfig()\n",
    "# model = ClapModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de98b57-74a0-464b-bc54-0ecda2367b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sampling_rate = librosa.load(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c082f-0951-48f8-95b1-f6aab724f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(0)\n",
    "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "\n",
    "# inputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\").to(0)\n",
    "# audio_embed = model.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f79f3-4763-4eb5-91b4-89ed02ed32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audios=audio_data, return_tensors=\"pt\").to(0)\n",
    "audio_embed = model.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2726c-548c-450c-88ce-ad2bc26eb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914d458-b9b4-459f-b01d-5afbc291931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec237e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting moviepy\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m645.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting decorator<5.0,>=4.0.2 (from moviepy)\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from moviepy) (2.28.2)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from moviepy) (1.24.2)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Obtaining dependency information for imageio<3.0,>=2.5 from https://files.pythonhosted.org/packages/f6/37/e21e6f38b93878ba80302e95b8ccd4718d80f0c53055ccae343e606b1e2d/imageio-2.31.5-py3-none-any.whl.metadata\n",
      "  Downloading imageio-2.31.5-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Obtaining dependency information for imageio_ffmpeg>=0.2.0 from https://files.pythonhosted.org/packages/1a/98/3df1d8dd8f2c121b6c588b1e0d604f36592d56df9c41fb155ed546c6a5ed/imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (9.0.0)\n",
      "Requirement already satisfied: setuptools in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (68.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/media/wiseyak/reasoning/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.12.7)\n",
      "Downloading imageio-2.31.5-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110721 sha256=a9f46c99981287d3a46afffeca6f1ba1d2e1867f291ba59171728b3d78bf8092\n",
      "  Stored in directory: /home/optimus/.cache/pip/wheels/e4/a4/db/0368d3a04033da662e13926594b3a8cf1aa4ffeefe570cfac1\n",
      "Successfully built moviepy\n",
      "Installing collected packages: proglog, imageio_ffmpeg, imageio, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-2.31.5 imageio_ffmpeg-0.4.9 moviepy-1.0.3 proglog-0.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a6346c-215d-4047-bad5-4c1ef3d19401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "from moviepy.editor import AudioFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3045c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/x-wav;base64,UklGRkYAAABXQVZFZm10IBAAAAABAAIARKwAABCxAgAEABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGEAAAAA\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Audio(\"datasets/AudioCaps/train/100011.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79375dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_duration(file_path):\n",
    "    audio_clip = AudioFileClip(file_path)\n",
    "    duration = audio_clip.duration\n",
    "    audio_clip.close()\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d2415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio_data, sampling_rate = librosa.load(\"datasets/AudioCaps/train/59\n",
    "get_audio_duration(\"datasets/AudioCaps/train/0.wav\") == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5dd61e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.wav\n",
      "100004.wav\n",
      "1\n",
      "100005.wav\n",
      "100011.wav\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "MoviePy error: failed to read the duration of file datasets/AudioCaps/train/100011.wav.\nHere are the file infos returned by ffmpeg:\n\nffmpeg version 4.2.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2019 the FFmpeg developers\n  built with gcc 8 (Debian 8.3.0-6)\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n[wav @ 0x69cc480] Cannot check for SPDIF\nGuessed Channel Layout for Input Stream #0.0 : stereo\nInput #0, wav, from 'datasets/AudioCaps/train/100011.wav':\n  Metadata:\n    encoder         : Lavf58.29.100\n  Duration: N/A, bitrate: 1411 kb/s\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\nAt least one output file must be specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/media/wiseyak/reasoning/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_reader.py:286\u001b[0m, in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    285\u001b[0m line \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m keyword \u001b[39min\u001b[39;00m l][index]\n\u001b[0;32m--> 286\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mfindall(\u001b[39m\"\u001b[39;49m\u001b[39m([0-9][0-9]:[0-9][0-9]:[0-9][0-9].[0-9][0-9])\u001b[39;49m\u001b[39m\"\u001b[39;49m, line)[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    287\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cvsecs(match)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m index,f \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mdatasets/AudioCaps/train\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(f)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m get_audio_duration(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m\"\u001b[39;49m\u001b[39mdatasets/AudioCaps/train\u001b[39;49m\u001b[39m\"\u001b[39;49m,f)) \u001b[39m!=\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39mprint\u001b[39m(count)\n",
      "\u001b[1;32m/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_audio_duration\u001b[39m(file_path):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     audio_clip \u001b[39m=\u001b[39m AudioFileClip(file_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     duration \u001b[39m=\u001b[39m audio_clip\u001b[39m.\u001b[39mduration\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.34.2.94/home/wiseyak/aafiya/gill/GILL_Inference_Examples.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     audio_clip\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/mnt/media/wiseyak/reasoning/lib/python3.8/site-packages/moviepy/audio/io/AudioFileClip.py:70\u001b[0m, in \u001b[0;36mAudioFileClip.__init__\u001b[0;34m(self, filename, buffersize, nbytes, fps)\u001b[0m\n\u001b[1;32m     67\u001b[0m AudioClip\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m FFMPEG_AudioReader(filename, fps\u001b[39m=\u001b[39;49mfps, nbytes\u001b[39m=\u001b[39;49mnbytes,\n\u001b[1;32m     71\u001b[0m                                  buffersize\u001b[39m=\u001b[39;49mbuffersize)\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps \u001b[39m=\u001b[39m fps\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mduration\n",
      "File \u001b[0;32m/mnt/media/wiseyak/reasoning/lib/python3.8/site-packages/moviepy/audio/io/readers.py:51\u001b[0m, in \u001b[0;36mFFMPEG_AudioReader.__init__\u001b[0;34m(self, filename, buffersize, print_infos, fps, nbytes, nchannels)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macodec \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpcm_s\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(\u001b[39m8\u001b[39m\u001b[39m*\u001b[39mnbytes)\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnchannels \u001b[39m=\u001b[39m nchannels\n\u001b[0;32m---> 51\u001b[0m infos \u001b[39m=\u001b[39m ffmpeg_parse_infos(filename)\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration \u001b[39m=\u001b[39m infos[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mvideo_duration\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m infos:\n",
      "File \u001b[0;32m/mnt/media/wiseyak/reasoning/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_reader.py:289\u001b[0m, in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    287\u001b[0m         result[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cvsecs(match)\n\u001b[1;32m    288\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m((\u001b[39m\"\u001b[39m\u001b[39mMoviePy error: failed to read the duration of file \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mHere are the file infos returned by ffmpeg:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m%\u001b[39m(\n\u001b[1;32m    291\u001b[0m                           filename, infos))\n\u001b[1;32m    293\u001b[0m \u001b[39m# get the output line that speaks about video\u001b[39;00m\n\u001b[1;32m    294\u001b[0m lines_video \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m Video: \u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m l \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+x\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, l)]\n",
      "\u001b[0;31mOSError\u001b[0m: MoviePy error: failed to read the duration of file datasets/AudioCaps/train/100011.wav.\nHere are the file infos returned by ffmpeg:\n\nffmpeg version 4.2.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2019 the FFmpeg developers\n  built with gcc 8 (Debian 8.3.0-6)\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n[wav @ 0x69cc480] Cannot check for SPDIF\nGuessed Channel Layout for Input Stream #0.0 : stereo\nInput #0, wav, from 'datasets/AudioCaps/train/100011.wav':\n  Metadata:\n    encoder         : Lavf58.29.100\n  Duration: N/A, bitrate: 1411 kb/s\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\nAt least one output file must be specified\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index,f in enumerate(os.listdir(\"datasets/AudioCaps/train\")):\n",
    "    print(f)\n",
    "    if get_audio_duration(os.path.join(\"datasets/AudioCaps/train\",f)) != 10:\n",
    "        count += 1\n",
    "        print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a6711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
