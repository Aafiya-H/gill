{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# GILL Inference Examples\n",
    "\n",
    "This is a notebook showcasing how to run GILL for image generation, image retrieval, and text generation, some of the tasks that GILL is capable of. It reproduces several examples in our paper, [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216).\n",
    "\n",
    "For reproducibility, all examples in this notebook use greedy (deterministic) decoding. However, it is possible to change to nucleus sampling for more diverse and higher quality outputs (used for some of the figures in the paper) by changing the `temperature` and `top_p` parameters in the `generate()` function.\n",
    "\n",
    "At least 22GB of GPU memory is required to run this model, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import notebook\n",
    "\n",
    "from gill import models\n",
    "from gill import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f36e5f-7027-4e60-95d5-05aa43ee34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1463559",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Note that you will need to download the [CC3M image embeddings](https://drive.google.com/file/d/1e9Cimh2dpWN8Cbgx_mSR-954Dr-DS-ZO/view) and place them in the `checkpoints/gill_opt/` folder, in order to use GILL's image retrieval capabilities. If this embedding file does not exist, the model will still run, but it will exclusively generate images (as opposed to deciding when to retrieve or generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc3m.npy files do not exist in checkpoints/gill_opt/.\n",
      "Running the model without retrieval.\n",
      "Adding [IMG0] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG0]\") = {'input_ids': [10975, 3755, 534, 288, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG0]\") = {'input_ids': [50266], 'attention_mask': [1]}\n",
      "Adding [IMG1] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG1]\") = {'input_ids': [10975, 3755, 534, 134, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG1]\") = {'input_ids': [50267], 'attention_mask': [1]}\n",
      "Adding [IMG2] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG2]\") = {'input_ids': [10975, 3755, 534, 176, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG2]\") = {'input_ids': [50268], 'attention_mask': [1]}\n",
      "Adding [IMG3] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG3]\") = {'input_ids': [10975, 3755, 534, 246, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG3]\") = {'input_ids': [50269], 'attention_mask': [1]}\n",
      "Adding [IMG4] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG4]\") = {'input_ids': [10975, 3755, 534, 306, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG4]\") = {'input_ids': [50270], 'attention_mask': [1]}\n",
      "Adding [IMG5] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG5]\") = {'input_ids': [10975, 3755, 534, 245, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG5]\") = {'input_ids': [50271], 'attention_mask': [1]}\n",
      "Adding [IMG6] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG6]\") = {'input_ids': [10975, 3755, 534, 401, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG6]\") = {'input_ids': [50272], 'attention_mask': [1]}\n",
      "Adding [IMG7] token to vocabulary.\n",
      "Before adding new token, tokenizer(\"[IMG7]\") = {'input_ids': [10975, 3755, 534, 406, 742], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "After adding 1 new tokens, tokenizer(\"[IMG7]\") = {'input_ids': [50273], 'attention_mask': [1]}\n",
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/media/wiseyak/reasoning/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for laion/clap-htsat-fused.\n",
      "Using facebook/opt-125m for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 4 visual tokens.\n",
      "Using laion/clap-htsat-fused as audio encoder\n",
      "Freezing the audio model\n",
      "Freezing the LM.\n",
      "Restoring pretrained weights for the visual model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'visual_projection.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_projection.weight', 'text_model.encoder.layers.0.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/media/wiseyak/reasoning/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the VM.\n",
      "---------- Audio embedding ----------\n",
      "LM input embedding dimension :  768\n",
      "number of visual tokens :  4\n",
      "Embedding dim:  3072\n",
      "Retrieval embedding dim:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decision model...\n"
     ]
    }
   ],
   "source": [
    "# Download the model checkpoint and embeddings to checkpoints/gill_opt/\n",
    "model_dir = 'checkpoints/gill_opt/'\n",
    "model = models.load_gill(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040de82-8305-4587-a340-4ae95401c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caption\n",
    "encoder_outputs = torch.rand((1,1024))\n",
    "hidden_size = 1024\n",
    "\n",
    "visual_embeddings = nn.Linear(hidden_size, 16384)\n",
    "visual_embs = visual_embeddings(encoder_outputs)\n",
    "visual_embs = torch.reshape(visual_embs, (visual_embs.shape[0], 4, -1))\n",
    "visual_embs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e575ef-11aa-4b4a-a617-aaaf789d45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retireval\n",
    "visual_fc = nn.Linear(hidden_size, 256)\n",
    "visual_embs = visual_fc(encoder_outputs)\n",
    "print(visual_embs.size())\n",
    "visual_embs = torch.reshape(visual_embs, (visual_embs.shape[0], 1, -1))\n",
    "print(visual_embs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66584911",
   "metadata": {},
   "source": [
    "# Image Generation\n",
    "\n",
    "GILL can generate images conditioned on image and text inputs. Shown are several examples for various text prompts and image + text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c9fa0-e417-447f-bfbe-d783d39a2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "red = utils.get_image_from_url('https://unsplash.com/photos/3TuIIkWlpvA/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8cmVkJTIwY29sb3J8ZW58MHx8fHwxNjkxOTk1NzM0fDA&force=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5eb4-10c0-4d10-96cc-2af5ff64909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# response = requests.get('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "# ['an astronaut riding a horse on mars'],\n",
    "    # [sofa_img, 'a picture of this but in red, color'],\n",
    "\n",
    "# Generate for a few types of text prompts and image + text prompts.\n",
    "for prompt in [\n",
    "    ['a picture of a cat'],    \n",
    "]:\n",
    "    g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "    return_outputs = model.generate_for_images_and_texts(\n",
    "        prompt, num_words=2, ret_scale_factor=100.0, generator=g_cuda)\n",
    "    \n",
    "    # Show either the generated or retrieved image, depending on the decision model outputs.\n",
    "    if return_outputs[1]['decision'][0] == 'gen':\n",
    "        plt.imshow(return_outputs[1]['gen'][0][0])\n",
    "        plt.title('Generated')\n",
    "    else:\n",
    "        plt.imshow(return_outputs[1]['ret'][0][0].resize((512, 512)))\n",
    "        plt.title(f\"Retrieved\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92172951",
   "metadata": {},
   "source": [
    "# Caption an audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a386f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_data, sampling_rate = librosa.load(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c693ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "type(audio_data) in [np.ndarray,torch.Tensor,List[np.ndarray],List[torch.Tensor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fce2bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Audio embs shape (captioning) : torch.Size([1, 4, 768])\n",
      "<class 'str'>\n",
      " a man who has been in a coma for a year.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    audio_data,\n",
    "    'This is the sound of'\n",
    "]\n",
    "\n",
    "return_outputs = model.generate_for_images_and_texts(prompts, num_words=16, min_word_tokens=16)\n",
    "print(return_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee0ce",
   "metadata": {},
   "source": [
    "# Multimodal Dialogue\n",
    "\n",
    "GILL can also generate dialogue-like text. We define some helper functions for displaying outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3a339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_dialogue(prompts: list, system_message: str = None, num_words: int = 32,\n",
    "                      sf: float = 1.0, temperature: float = 0.0, top_p: float = 1.0,\n",
    "                      divider_count: int = 40):\n",
    "    g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "\n",
    "    full_outputs = []\n",
    "    if system_message:\n",
    "        print(\"Adding system message\")\n",
    "        full_inputs = [system_message]\n",
    "    else:\n",
    "        full_inputs = []\n",
    "\n",
    "    for prompt_idx, prompt in notebook.tqdm(enumerate(prompts), total=len(prompts)):\n",
    "        formatted_prompt = []\n",
    "        for p in prompt:\n",
    "            if type(p) == Image.Image:\n",
    "                full_inputs.append(p)\n",
    "                formatted_prompt.append(p)\n",
    "            elif type(p) == str:\n",
    "                full_inputs.append(f'Q: {p}\\nA:')\n",
    "                formatted_prompt.append(f'User: {p}')\n",
    "        formatted_prompt.append('=' * divider_count)  # Add divider\n",
    "\n",
    "        return_outputs = model.generate_for_images_and_texts(\n",
    "            full_inputs, num_words=num_words, ret_scale_factor=sf,\n",
    "            generator=g_cuda, temperature=temperature, top_p=top_p)\n",
    "\n",
    "        # Add outputs\n",
    "        output_text = return_outputs[0].replace('[IMG0] [IMG1] [IMG2] [IMG3] [IMG4] [IMG5] [IMG6] [IMG7]', '')\n",
    "        full_inputs.append(output_text + '\\n')\n",
    "\n",
    "        formatted_return_outputs = []\n",
    "        for p in return_outputs:\n",
    "            if type(p) == str:\n",
    "                p_formatted = p.replace('[IMG0] [IMG1] [IMG2] [IMG3] [IMG4] [IMG5] [IMG6] [IMG7]', '')\n",
    "                formatted_return_outputs.append(f'GILL: {p_formatted}')\n",
    "            else:\n",
    "                formatted_return_outputs.append(p)\n",
    "        formatted_return_outputs.append('=' * divider_count)  # Add divider\n",
    "\n",
    "        full_outputs.extend(formatted_prompt + formatted_return_outputs)\n",
    "\n",
    "    return full_outputs\n",
    "\n",
    "\n",
    "def display_conversation(full_outputs):\n",
    "    # Display conversation.\n",
    "    for p in full_outputs:\n",
    "        if type(p) == Image.Image:\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(p)\n",
    "            plt.show()\n",
    "        elif type(p) == str:\n",
    "            print(p)\n",
    "        elif type(p) == dict:\n",
    "            # Decide whether to retrieve or generate\n",
    "            decision_probs = [f'{s:.3f}' for s in p['decision'][1]]\n",
    "            if p['decision'][0] == 'gen':\n",
    "                gen_img = p['gen'][0][0].resize((512, 512))\n",
    "                # Generate\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.imshow(gen_img)\n",
    "                plt.title(f'GENERATED (p={decision_probs})')\n",
    "            else:\n",
    "                ret_img = p['ret'][0][0].resize((512, 512))\n",
    "                # Retrieve\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.imshow(ret_img)\n",
    "                plt.title(f'RETRIEVED (p={decision_probs})')\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise NotImplementedError(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73049d9b",
   "metadata": {},
   "source": [
    "The inputs to the model can be an interleaved image and text sequence. Shown is one example from our paper, with a cupcake image input and a question (note that the word \"cupcake\" is never explicitly mentioned, but GILL infers it from the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        utils.get_image_from_url('https://www.allrecipes.com/thmb/riDYvmalWk8QgJDBT_pZRkpfpR0=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/17377-chocolate-cupcakes-DDMFS-4x3-622a7a66fcd84692947794ed385dc991.jpg'),\n",
    "        'How should I publicise these at the market?'\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e5c28",
   "metadata": {},
   "source": [
    "# Image-to-Text Example\n",
    "\n",
    "GILL can also generate text conditioned on image and text inputs. This is helpful for tasks such as image captioning or VQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87889b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pancakes_img = utils.get_image_from_url('https://images.pexels.com/photos/376464/pexels-photo-376464.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "prompts = [\n",
    "    pancakes_img,\n",
    "    'A picture of'\n",
    "]\n",
    "\n",
    "plt.imshow(pancakes_img)\n",
    "plt.show()\n",
    "\n",
    "return_outputs = model.generate_for_images_and_texts(prompts, num_words=16, min_word_tokens=16)\n",
    "print(return_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8ee5e-a37a-4b77-b638-0508335e5af7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interleaving Image and Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb33973-51e8-46db-bf9d-76c85de77667",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "pancake = utils.get_image_from_url('https://images.pexels.com/photos/376464/pexels-photo-376464.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "whipped_cream = utils.get_image_from_url(\"https://media.istockphoto.com/id/510175498/photo/whipped-cream-on-white-background.jpg?s=2048x2048&w=is&k=20&c=Z_4UJPvUq2kmQXnWngY6apzKZjDgRIGfxaa82mPakcM=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7e32a-671c-45ab-80e3-c366811bcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [pancake, \"This but with grey colored syrup\"]\n",
    "# g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
    "# return_outputs = model.generate_for_images_and_texts(prompts, num_words=2, ret_scale_factor=100.0, generator=g_cuda)\n",
    "\n",
    "# # Show either the generated or retrieved image, depending on the decision model outputs.\n",
    "# if return_outputs[1]['decision'][0] == 'gen':\n",
    "#     plt.imshow(return_outputs[1]['gen'][0][0])\n",
    "#     plt.title('Generated')\n",
    "# else:\n",
    "#     plt.imshow(return_outputs[1]['ret'][0][0].resize((512, 512)))\n",
    "#     plt.title(f\"Retrieved\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46979494-610b-4ef7-afa9-4a215d22f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_img = utils.get_image_from_url('https://images.pexels.com/photos/1866149/pexels-photo-1866149.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n",
    "blue = utils.get_image_from_url('https://unsplash.com/photos/0YQz7M2fcYY/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjkyMDEyMzA5fA&force=true')\n",
    "red = utils.get_image_from_url('https://unsplash.com/photos/3TuIIkWlpvA/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8cmVkJTIwY29sb3J8ZW58MHx8fHwxNjkxOTk1NzM0fDA&force=true')\n",
    "\n",
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        sofa_img, \" but in \", blue, \" color.\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb8b85-4a79-4227-a9d9-fa07088a30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1.4  # Scaling factor: increase to increase the chance of returning an image\n",
    "temperature = 0.0  # 0 means deterministic, try 0.6 for more randomness\n",
    "top_p = 1.0  # If you set temperature to 0.6, set this to 0.95\n",
    "num_words = 32\n",
    "\n",
    "prompts = [\n",
    "    [\n",
    "        \"How would this \", pancake, \" look with \", whipped_cream, \" on it?\"\n",
    "    ],\n",
    "]\n",
    "\n",
    "full_outputs = generate_dialogue(prompts, num_words=num_words, sf=sf, temperature=temperature, top_p=top_p)\n",
    "display_conversation(full_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647dd5a5-6547-4825-a353-9f44bd6b2eef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VIST eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a19add",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://visionandlanguage.net/VIST/json_files/story-in-sequence/SIS-with-labels.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d61d20-8e4f-457f-a877-42a5b115a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf SIS-with-labels.tar.gz -C sis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98711c54-8b0f-4b64-aabf-fe31c4688c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm SIS-with-labels.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a868cc-990d-4aaa-a8a9-711832e5893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evals/download_vist_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72dd40-df6a-449c-ab1a-29e29949f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "%%writefile output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb466c1-9e9d-4275-b297-ad670292b779",
   "metadata": {},
   "source": [
    "## Audio Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d12e3-c267-4686-ac6f-e53051fa15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from transformers import ClapModel, ClapProcessor, ClapConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3505760-8857-40e4-ac38-b929f166efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feba2bb-c458-48e5-8c3a-5f174d3ab9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration = ClapConfig()\n",
    "# model = ClapModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de98b57-74a0-464b-bc54-0ecda2367b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, sampling_rate = librosa.load(\"sin_100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c082f-0951-48f8-95b1-f6aab724f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(0)\n",
    "processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n",
    "\n",
    "# inputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\").to(0)\n",
    "# audio_embed = model.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f79f3-4763-4eb5-91b4-89ed02ed32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audios=audio_data, return_tensors=\"pt\").to(0)\n",
    "audio_embed = model.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2726c-548c-450c-88ce-ad2bc26eb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914d458-b9b4-459f-b01d-5afbc291931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6346c-215d-4047-bad5-4c1ef3d19401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
